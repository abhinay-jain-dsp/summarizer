{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.utils import pad_sequences, Sequence\n",
    "from keras.preprocessing.text import Tokenizer, tokenizer_from_json\n",
    "import tensorflow as tf\n",
    "import shutil\n",
    "import json\n",
    "import re\n",
    "import math\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "from psycopg2 import Error as PGError\n",
    "\n",
    "import importlib\n",
    "atUtils = importlib.import_module('at-utils')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "batchSize = 128\n",
    "numWords = 10000\n",
    "inputMaxLen = 256\n",
    "outputMaxLen = 64\n",
    "embeddingDimension = 100\n",
    "testSplit = 0.1\n",
    "valSplit = 0.1\n",
    "hiddenDim = inputMaxLen + outputMaxLen\n",
    "epochs = 4\n",
    "\n",
    "bos = 'beginningofsentence'\n",
    "eos = 'endofsentence'\n",
    "GLOVE_FILE = '/home/ston/glove.6B.100d.txt'\n",
    "tableName = 'reddit'\n",
    "tokenizerJsonPath = 'tokenizer.summarizer.json'\n",
    "modelPath = 'summarizer-model.keras'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db connection\n",
    "connectionParams = {\n",
    "    'dbname': 'nlp',\n",
    "    'user': 'tf',\n",
    "    'password': 'wasd',\n",
    "    'host': 'localhost',\n",
    "    'port': '5432'\n",
    "}\n",
    "def connectToPg():\n",
    "    return psycopg2.connect(**connectionParams)\n",
    "connection = connectToPg()\n",
    "\n",
    "def executeSelectQuery(query):\n",
    "    try:\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(query)\n",
    "        rows = cursor.fetchall()\n",
    "\n",
    "        cursor.close()\n",
    "\n",
    "        return rows\n",
    "\n",
    "    except (Exception, PGError) as error:\n",
    "        print(\"Error while connecting to PostgreSQL\", error)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read json and import to db\n",
    "# tldrRegex = r\"tld?\\s?[;:.,'|\\_\\-\\\\\\/]{0,2}\\s?dr\"\n",
    "# def validateComment(comment, requiredKeys):\n",
    "#     for key in requiredKeys:\n",
    "#         if key not in comment.keys():\n",
    "#             return False\n",
    "#     if not re.search(tldrRegex, comment['body'], re.IGNORECASE):\n",
    "#         return False\n",
    "#     return True\n",
    "\n",
    "# def checkForNewKeys(comment, knownKeys):\n",
    "#     for key in comment.keys():\n",
    "#         if key not in knownKeys:\n",
    "#             print(key)\n",
    "\n",
    "# insertQuery = \"\"\"\n",
    "#     INSERT INTO reddit\n",
    "#     (reddit_id, author, title, body, normalized_body, content, summary, content_len, summary_len, subreddit, subreddit_id)\n",
    "#     VALUES %s\n",
    "# \"\"\"\n",
    "# def saveBatch(batchComments):\n",
    "#     cur = conn.cursor()\n",
    "#     execute_values(cur, insertQuery, batchComments)\n",
    "#     conn.commit()\n",
    "#     cur.close()\n",
    "\n",
    "# def importData(filePath = '/home/ston/reddit.json'):\n",
    "#     f = open(filePath, mode='r')\n",
    "#     requiredKeys = [\n",
    "#         'id',\n",
    "#         'author',\n",
    "#         'body',\n",
    "#         'normalizedBody',\n",
    "#         'content',\n",
    "#         'summary',\n",
    "#         'content_len',\n",
    "#         'summary_len',\n",
    "#         'subreddit',\n",
    "#         'subreddit_id'\n",
    "#     ]\n",
    "#     # knownKeys = list(requiredKeys) + ['title']\n",
    "#     batchSize = 10000\n",
    "#     batchCount = 0\n",
    "#     while True:\n",
    "#         batchComments = []\n",
    "\n",
    "#         for _ in range(batchSize):\n",
    "#             line = f.readline()\n",
    "#             if not line:\n",
    "#                 break\n",
    "#             comment = json.loads(line)\n",
    "#             # checkForNewKeys(comment, knownKeys)\n",
    "#             if not validateComment(comment, requiredKeys):\n",
    "#                 continue\n",
    "#             batchComments.append((\n",
    "#                 comment['id'],\n",
    "#                 comment['author'],\n",
    "#                 comment['title'] if 'title' in comment else None,\n",
    "#                 comment['body'],\n",
    "#                 comment['normalizedBody'],\n",
    "#                 comment['content'],\n",
    "#                 comment['summary'],\n",
    "#                 comment['content_len'],\n",
    "#                 comment['summary_len'],\n",
    "#                 comment['subreddit'],\n",
    "#                 comment['subreddit_id']\n",
    "#             ))\n",
    "\n",
    "#         if len(batchComments) == 0:\n",
    "#             break\n",
    "\n",
    "#         batchCount += 1\n",
    "#         print(f'processing batch no: {batchCount}. batch size: {len(batchComments)}')\n",
    "#         saveBatch(batchComments)\n",
    "\n",
    "# importData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dict and write to file\n",
    "# def createVocab(numWords):\n",
    "#     tokenizer = Tokenizer(num_words = numWords)\n",
    "#     tBatchSize = 409600\n",
    "#     rowCount = executeSelectQuery('select count(*) from reddit')[0][0]\n",
    "#     tBatchCount = int(np.ceil(rowCount / tBatchSize))\n",
    "\n",
    "#     for i in range(tBatchCount):\n",
    "#         print(i)\n",
    "#         rows = executeSelectQuery(f'''\n",
    "#             select\n",
    "#                 content,\n",
    "#                 summary\n",
    "#             from reddit\n",
    "#             where id between {i * tBatchSize + 1} and {(i + 1) * tBatchSize}\n",
    "#         ''')\n",
    "\n",
    "#         texts = [f'{bos} {text} {eos}' for tup in rows for text in tup]\n",
    "#         tokenizer.fit_on_texts(texts)\n",
    "\n",
    "#     return tokenizer\n",
    "\n",
    "\n",
    "# tokenizer = createVocab(numWords)\n",
    "# with open(tokenizerJsonPath, 'w') as f:\n",
    "#     data = tokenizer.to_json()\n",
    "#     f.write(json.dumps(data))\n",
    "#     f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read tokenizer from file\n",
    "with open(tokenizerJsonPath, 'r') as f:\n",
    "    data = json.load(f)\n",
    "    tokenizer = tokenizer_from_json(data)\n",
    "    f.close()\n",
    "\n",
    "numWords = min(len(tokenizer.word_index.keys()) + 1, numWords)\n",
    "wordToIdx, idxToWord = atUtils.getTokenizerDicts(tokenizer, numWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        idStart = 0,\n",
    "        tableName = tableName,\n",
    "        inputMaxLen = inputMaxLen,\n",
    "        outputMaxLen = outputMaxLen,\n",
    "        tokenizer = tokenizer,\n",
    "        cleanTexts = atUtils.cleanTexts,\n",
    "        padding = atUtils.padding,\n",
    "        getDecoderOutput = atUtils.getDecoderOutput,\n",
    "        executeSelectQuery = executeSelectQuery,\n",
    "        batchSize = batchSize,\n",
    "        numWords = numWords,\n",
    "        cacheSize = 2048\n",
    "    ):\n",
    "        self.shuffle = False\n",
    "        self.idStart = idStart\n",
    "        self.dataset = dataset\n",
    "        self.tableName = tableName\n",
    "        self.inputMaxLen = inputMaxLen\n",
    "        self.outputMaxLen = outputMaxLen\n",
    "        self.tokenizer = tokenizer\n",
    "        self.cleanTexts = cleanTexts\n",
    "        self.padding = padding\n",
    "        self.getDecoderOutput = getDecoderOutput\n",
    "        self.executeSelectQuery = executeSelectQuery\n",
    "        self.batchSize = batchSize\n",
    "        self.numWords = numWords\n",
    "\n",
    "        rowCount = executeSelectQuery(f'''\n",
    "            select\n",
    "                count(*)\n",
    "            from reddit\n",
    "            where\n",
    "                dataset = {dataset}\n",
    "                --and include_in_training = true\n",
    "        ''')[0][0]\n",
    "        self.batchCount = int(np.ceil(rowCount / batchSize))\n",
    "        self.cacheSize = cacheSize + batchSize + cacheSize % batchSize\n",
    "        self.cache = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.batchCount\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if not len(self.cache):\n",
    "            self.populateCache(index)\n",
    "\n",
    "        rows, self.cache = self.cache[0:self.batchSize], self.cache[self.batchSize:]\n",
    "        enIn = []\n",
    "        deIn = []\n",
    "        for row in rows:\n",
    "            e, d = row[0], row[1]\n",
    "            d = f'{bos} {d} {eos}'\n",
    "            enIn.append(e)\n",
    "            deIn.append(d)\n",
    "\n",
    "        enIn = self.textToEncodedInput(enIn, self.inputMaxLen)\n",
    "        deIn = self.textToEncodedInput(deIn, self.outputMaxLen)\n",
    "        deO = self.getDecoderOutput(deIn, self.outputMaxLen)\n",
    "\n",
    "        return [enIn, deIn], deO\n",
    "\n",
    "    def textToEncodedInput(self, texts, maxLen):\n",
    "        texts = self.cleanTexts(texts)\n",
    "        seqs = self.tokenizer.texts_to_sequences(texts)\n",
    "        seqs = self.padding(seqs, maxLen)\n",
    "        return seqs\n",
    "\n",
    "    def populateCache(self, index):\n",
    "        startIndex = self.idStart + index * self.batchSize + 1\n",
    "        endIndex = startIndex + self.cacheSize - 1\n",
    "        self.cache = self.executeSelectQuery(f'''\n",
    "            select\n",
    "                content,\n",
    "                summary\n",
    "            from reddit\n",
    "            where\n",
    "                id between {startIndex} and {endIndex}\n",
    "                and dataset = {self.dataset}\n",
    "                --and include_in_training = true\n",
    "        ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getModel(hiddenDim):\n",
    "#     encoderInputs = Input(shape = (None,), dtype = 'float32', name = 'encoderInputs')\n",
    "#     encoderEmbeddingLayer = atUtils.getEmbeddingLayer(GLOVE_FILE, numWords, embeddingDimension, inputMaxLen, wordToIdx, 'encoderEmbeddingLayer')\n",
    "#     encoderEmbedding = encoderEmbeddingLayer(encoderInputs)\n",
    "#     encoderLSTM = LSTM(hiddenDim, return_state=True, name = 'encoderLSTM')\n",
    "#     _, stateH, stateC = encoderLSTM(encoderEmbedding)\n",
    "\n",
    "#     decoderInputs = Input(shape = (None,), dtype = 'float32', name = 'decoderInputs')\n",
    "#     decoderEmbeddingLayer = atUtils.getEmbeddingLayer(GLOVE_FILE, numWords, embeddingDimension, outputMaxLen, wordToIdx, 'decoderEmbeddingLayer')\n",
    "#     decoderEmbedding = decoderEmbeddingLayer(decoderInputs)\n",
    "#     decoderLSTM = LSTM(hiddenDim, return_state=True, return_sequences=True, name = 'decoderLSTM')\n",
    "#     decoderOutputs, _, _ = decoderLSTM(decoderEmbedding, initial_state=[stateH, stateC])\n",
    "\n",
    "#     denseLayer = Dense(numWords, activation='softmax', name = 'denseLayer')\n",
    "#     outputs = denseLayer(decoderOutputs)\n",
    "#     model = Model([encoderInputs, decoderInputs], outputs)\n",
    "\n",
    "#     return model\n",
    "\n",
    "# model = getModel(hiddenDim)\n",
    "# model.compile(\n",
    "#     optimizer='adam',\n",
    "#     loss='sparse_categorical_crossentropy',\n",
    "#     metrics=['sparse_categorical_accuracy']\n",
    "# )\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(modelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainDataGen = DataGenerator(dataset = 0)\n",
    "# valDataGen = DataGenerator(dataset = 1, idStart = 3000001)\n",
    "\n",
    "# model.fit(\n",
    "#     trainDataGen,\n",
    "#     epochs = epochs,\n",
    "#     batch_size = batchSize,\n",
    "#     validation_data = valDataGen\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(modelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeInferenceModels():\n",
    "    encoderInputs = model.get_layer('encoderInputs').output\n",
    "    encoderEmbeddingLayer = model.get_layer('encoderEmbeddingLayer')\n",
    "    encoderEmbedded = encoderEmbeddingLayer(encoderInputs)\n",
    "    encoderLSTM = model.get_layer('encoderLSTM')\n",
    "    _, hEnc, state_cEnc = encoderLSTM(encoderEmbedded)\n",
    "    encoderStates = [hEnc, state_cEnc]\n",
    "    encoderModel = Model(encoderInputs, encoderStates)\n",
    "\n",
    "    decoderInputs = model.get_layer('decoderInputs').output\n",
    "    decoderEmbeddingLayer = model.get_layer('decoderEmbeddingLayer')\n",
    "    decoderEmbedded = decoderEmbeddingLayer(decoderInputs)\n",
    "    hDecInput = Input(shape=(hiddenDim,))\n",
    "    cDecInput = Input(shape=(hiddenDim,))\n",
    "    decoderLSTM = model.get_layer('decoderLSTM')\n",
    "    decoderOutputs, hDec, cDec = decoderLSTM(\n",
    "        decoderEmbedded,\n",
    "        initial_state=[hDecInput, cDecInput]\n",
    "    )\n",
    "    decoderDense = model.get_layer('denseLayer')\n",
    "    outputs = decoderDense(decoderOutputs)\n",
    "    decoderModel = Model(\n",
    "        [decoderInputs, hDecInput, cDecInput],\n",
    "        [outputs, hDec, cDec]\n",
    "    )\n",
    "\n",
    "    return encoderModel, decoderModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoderModel, decoderModel = makeInferenceModels()\n",
    "\n",
    "def decodeSequence(inputSeq):\n",
    "    encoderModel, decoderModel = makeInferenceModels()\n",
    "    h, c = encoderModel.predict(inputSeq, verbose=0)\n",
    "\n",
    "    targetSeq = np.zeros((1, 1))\n",
    "    targetSeq[0, 0] = wordToIdx[bos]\n",
    "\n",
    "    decodedSentence = []\n",
    "    for _ in range(outputMaxLen):\n",
    "        outputTokens, h, c = decoderModel.predict(\n",
    "            [targetSeq, h, c], verbose=0\n",
    "        )\n",
    "\n",
    "        sampledWordIndex = np.argmax(outputTokens[0, -1, :])\n",
    "        sampledWord = idxToWord.get(sampledWordIndex)\n",
    "        if sampledWord is None:\n",
    "            sampledWord = '<OOD>'\n",
    "        decodedSentence.append(sampledWord)\n",
    "\n",
    "        targetSeq = np.zeros((1, 1))\n",
    "        targetSeq[0, 0] = sampledWordIndex\n",
    "\n",
    "        if sampledWord == eos or len(decodedSentence) > outputMaxLen:\n",
    "            break\n",
    "\n",
    "    return ' '.join(decodedSentence)\n",
    "\n",
    "\n",
    "def respondTo(message):\n",
    "    tokens = tokenizer.texts_to_sequences([message])\n",
    "    sequences = pad_sequences(\n",
    "        tokens,\n",
    "        maxlen = inputMaxLen,\n",
    "        dtype = 'int',\n",
    "        padding = 'post',\n",
    "        truncating = 'post'\n",
    "    )\n",
    "    return decodeSequence(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 48 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f500c602660> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'the industry is a business model and the industry is a business model and the industry is not a business model endofsentence'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# respondTo('hi how are you')\n",
    "# respondTo('how does chatgpt work')\n",
    "# respondTo('What happens in February')\n",
    "respondTo('''Engineers are creators, and the fascination of solving problems using tech is infectious,” says Yogesh Bhalla, CTO at DSP Asset Managers. He has always been captivated by the power of technology to address challenges, shaping his career trajectory. For Yogesh, success is defined by the four to five ambitious products he has created, the formidable problems he has solved, and the enduring satisfaction derived from choosing technology as a career.\n",
    "\n",
    "\"As a tech leader in the financial industry, Yogesh shoulders significant responsibilities, which mostly revolve around shaping three core ambitious products—JARVIS, RMX, and TITAN. These are carefully mapped out in a 1-year to 3-year roadmap. Yogesh deems to make these dream products successful by enabling alpha generation, streamlining processes across front, mid, and back offices, and increasing sales output. This goal is ambitious, aiming to contribute a 20 percent growth in sales assets under management (AUM) or to achieve and surpass alpha performance.\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
