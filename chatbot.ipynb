{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, LSTM, Dense, Embedding\n",
    "from keras.utils import pad_sequences, to_categorical, Sequence\n",
    "from keras.preprocessing.text import Tokenizer, tokenizer_from_json\n",
    "import tensorflow as tf\n",
    "import shutil\n",
    "import json\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "batchSize = 128\n",
    "numWords = 5000\n",
    "maxLen = 128\n",
    "embeddingDimension = 100\n",
    "testSplit = 0.1\n",
    "valSplit = 0.1\n",
    "hiddenDim = 100\n",
    "epochs = 128\n",
    "\n",
    "bos = 'beginningofsentence'\n",
    "eos = 'endofsentence'\n",
    "GLOVE_FILE = '/home/ston/glove.6B.100d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "def cleanText(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)\", \" \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"[-()'\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def cleanTexts(texts):\n",
    "    return [cleanText(text) for text in texts]\n",
    "\n",
    "def createVocab(textList, numWords):\n",
    "    tokenizer = Tokenizer(num_words = numWords)\n",
    "    tokenizer.fit_on_texts(textList)\n",
    "    return tokenizer\n",
    "\n",
    "def getTokenizerDicts(tokenizer, numWords):\n",
    "    wordToIdx = {}\n",
    "    idxToWord = {}\n",
    "    for k, v in tokenizer.word_index.items():\n",
    "        if v < numWords:\n",
    "            wordToIdx[k] = v\n",
    "            idxToWord[v] = k\n",
    "        if v >= numWords - 1:\n",
    "            continue\n",
    "    return wordToIdx, idxToWord\n",
    "\n",
    "def padding(sequences, maxLen):\n",
    "    return pad_sequences(\n",
    "        sequences,\n",
    "        maxlen = maxLen,\n",
    "        dtype = 'int',\n",
    "        padding = 'post',\n",
    "        truncating = 'post'\n",
    "    )\n",
    "\n",
    "def getDecoderOutput(decoderInput, maxLen):\n",
    "    decoderOutput = np.zeros((len(decoderInput), maxLen), dtype='float32')\n",
    "    for i, seq in enumerate(decoderInput):\n",
    "        decoderOutput[i] = np.append(seq[1:], 0.)\n",
    "\n",
    "    return decoderOutput\n",
    "\n",
    "def getEmbeddingLayer(numWords, embeddingDimension, maxLen, wordToIdx):\n",
    "    embeddingsIndex = {}\n",
    "    with open(GLOVE_FILE) as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddingsIndex[word] = coefs\n",
    "        f.close()\n",
    "\n",
    "    embeddingMatrix = np.zeros((len(wordToIdx) + 1, embeddingDimension), dtype='float32')\n",
    "    for word, i in wordToIdx.items():\n",
    "        embeddingVector = embeddingsIndex.get(word)\n",
    "        if embeddingVector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embeddingMatrix[i] = embeddingVector\n",
    "\n",
    "    return Embedding(\n",
    "        input_dim = numWords,\n",
    "        output_dim = embeddingDimension,\n",
    "        input_length = maxLen,\n",
    "        weights = [embeddingMatrix],\n",
    "        trainable = False,\n",
    "        mask_zero = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file\n",
    "# df = pd.read_csv(\"/home/ston/chatgpt-reddit-comments.csv\")\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic cleanup\n",
    "# df.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "# df.rename(columns = {\n",
    "#     'comment_id': 'reddit_id',\n",
    "#     'comment_parent_id': 'reddit_parent_id',\n",
    "#     'comment_body': 'body',\n",
    "# }, inplace = True)\n",
    "# df.dropna(inplace = True)\n",
    "\n",
    "# stripParentRegex = r'^t\\d_'\n",
    "# def stripParentId(id):\n",
    "#     if re.search(stripParentRegex, id, re.IGNORECASE):\n",
    "#         id = id[3:]\n",
    "#     else:\n",
    "#         print(id)\n",
    "#     return id\n",
    "# df['reddit_parent_id'] = df['reddit_parent_id'].apply(stripParentId)\n",
    "\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find comment and response pairs\n",
    "# def validateComment(comment):\n",
    "#     return comment['body'] != '[deleted]' and comment['body'] != '[removed]'\n",
    "# pairs = []\n",
    "# parentIds = set()\n",
    "# def appendPair(parent, row):\n",
    "#     if parent['reddit_id'] not in parentIds and validateComment(parent):\n",
    "#         parentIds.add(parent['reddit_id'])\n",
    "#         pairs.append((parent['body'], row['body']))\n",
    "\n",
    "# def findPairs(row):\n",
    "#     if not validateComment(row):\n",
    "#         pass\n",
    "#     parents = df[df['reddit_id'] == row['reddit_parent_id']]\n",
    "#     if len(parents):\n",
    "#         parents.apply(lambda parent: appendPair(parent, row), axis = 1)\n",
    "\n",
    "# df.apply(findPairs, axis = 1)\n",
    "# print(len(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write pairs to a file\n",
    "# with open('reddit-pairs.json', 'w') as f:\n",
    "#     json.dump(pairs, f)\n",
    "#     f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to line-by-line json for batch processing\n",
    "# shutil.move('reddit-pairs.json', 'reddit-pairs.bak.json')\n",
    "\n",
    "# with open('reddit-pairs.bak.json') as fSrc:\n",
    "#     pairs = json.load(fSrc)\n",
    "#     fSrc.close()\n",
    "\n",
    "# with open('reddit-pairs.json', 'w') as fOut:\n",
    "#     fOut.write('\\n'.join([json.dumps(pair) for pair in pairs]))\n",
    "#     fOut.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read X, Y pairs from file\n",
    "# with open('reddit-pairs.bak.json') as f:\n",
    "#    pairs = json.load(f)\n",
    "#    f.close()\n",
    "\n",
    "# X = []\n",
    "# Y = []\n",
    "# for tup in pairs:\n",
    "#     X.append(cleanText(tup[0]))\n",
    "#     Y.append(f'{bos} {cleanText(tup[1])} {eos}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and pad\n",
    "# tokenizer = createVocab(textList = X + Y, numWords = numWords)\n",
    "# numWords = min(len(tokenizer.word_index.keys()) + 1, numWords)\n",
    "# wordToIdx, idxToWord = getTokenizerDicts(tokenizer, numWords)\n",
    "\n",
    "# encoderInput = padding(tokenizer.texts_to_sequences(X), maxLen)\n",
    "# decoderInput = padding(tokenizer.texts_to_sequences(Y), maxLen)\n",
    "# decoder output (needs to be one-hot encoded)\n",
    "# decoderOutput = getDecoderOutput(decoderInput, maxLen, numWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write tokenizer to file\n",
    "# with open('tokenizer.json', 'w') as f:\n",
    "#     data = tokenizer.to_json()\n",
    "#     f.write(json.dumps(data))\n",
    "#     f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "# testSplitIndex = math.ceil(len(encoderInput) * (1 - testSplit))\n",
    "# enTrain, enTest = encoderInput[0:testSplitIndex], encoderInput[testSplitIndex:]\n",
    "# deTrain, deTest = decoderInput[0:testSplitIndex], decoderInput[testSplitIndex:]\n",
    "# deOTrain, deOTest = decoderOutput[0:testSplitIndex], decoderOutput[testSplitIndex:]\n",
    "\n",
    "# valSplitIndex = math.ceil(len(enTrain) * (1 - valSplit))\n",
    "# enTrain, enVal = enTrain[0:valSplitIndex], enTrain[valSplitIndex:]\n",
    "# deTrain, deVal = deTrain[0:valSplitIndex], deTrain[valSplitIndex:]\n",
    "# deOTrain, deOVal = deOTrain[0:valSplitIndex], deOTrain[valSplitIndex:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read prepared tokenizer\n",
    "with open('tokenizer.json') as f:\n",
    "    data = json.load(f)\n",
    "    tokenizer = tokenizer_from_json(data)\n",
    "    f.close()\n",
    "\n",
    "numWords = min(len(tokenizer.word_index.keys()) + 1, numWords)\n",
    "wordToIdx, idxToWord = getTokenizerDicts(tokenizer, numWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    def __init__(\n",
    "        self,\n",
    "        filePath = 'reddit-pairs.json',\n",
    "        tokenizer = tokenizer,\n",
    "        cleanTexts = cleanTexts,\n",
    "        padding = padding,\n",
    "        getDecoderOutput = getDecoderOutput,\n",
    "        batchSize = batchSize,\n",
    "        maxLen = maxLen,\n",
    "        numWords = numWords\n",
    "    ):\n",
    "        self.filePath = filePath\n",
    "        self.tokenizer = tokenizer\n",
    "        self.cleanTexts = cleanTexts\n",
    "        self.padding = padding\n",
    "        self.getDecoderOutput = getDecoderOutput\n",
    "        self.batchSize = batchSize\n",
    "        self.maxLen = maxLen\n",
    "        self.numWords = numWords\n",
    "\n",
    "        with open(filePath, 'r') as file:\n",
    "            lineCount = sum(1 for _ in file)\n",
    "            self.batchCount = int(np.ceil(lineCount / batchSize))\n",
    "            file.close()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.batchCount\n",
    "\n",
    "    def textToEncodedInput(self, texts):\n",
    "        texts = self.cleanTexts(texts)\n",
    "        seqs = self.tokenizer.texts_to_sequences(texts)\n",
    "        seqs = self.padding(seqs, self.maxLen)\n",
    "        return seqs\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        startIndex = index * self.batchSize\n",
    "        endIndex = (index + 1) * self.batchSize\n",
    "\n",
    "        file = open(self.filePath, 'r')\n",
    "        for _ in range(startIndex):\n",
    "            next(file)\n",
    "\n",
    "        enIn = []\n",
    "        deIn = []\n",
    "\n",
    "        for _ in range(startIndex, endIndex):\n",
    "            e, d = json.loads(file.readline())\n",
    "            d = f'{bos} {d} {eos}'\n",
    "            enIn.append(e)\n",
    "            deIn.append(d)\n",
    "\n",
    "        enIn = self.textToEncodedInput(enIn)\n",
    "        deIn = self.textToEncodedInput(deIn)\n",
    "        deO = self.getDecoderOutput(deIn, self.maxLen)\n",
    "\n",
    "        return [enIn, deIn], deO\n",
    "\n",
    "trainDataGen = DataGenerator(filePath = 'reddit-pairs.train.json')\n",
    "valDataGen = DataGenerator(filePath = 'reddit-pairs.validation.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel(hiddenDim):\n",
    "    embeddingLayer = getEmbeddingLayer(numWords, embeddingDimension, maxLen, wordToIdx)\n",
    "    encoderInputs = Input(shape = (None,), dtype = 'float32')\n",
    "    encoderEmbedding = embeddingLayer(encoderInputs)\n",
    "    encoderLSTM = LSTM(hiddenDim, return_state=True)\n",
    "    _, stateH, stateC = encoderLSTM(encoderEmbedding)\n",
    "\n",
    "    decoderInputs = Input(shape = (None,), dtype = 'float32')\n",
    "    decoderEmbedding = embeddingLayer(decoderInputs)\n",
    "    decoderLSTM = LSTM(hiddenDim, return_state=True, return_sequences=True)\n",
    "    decoderOutputs, _, _ = decoderLSTM(decoderEmbedding, initial_state=[stateH, stateC])\n",
    "\n",
    "    denseLayer = Dense(numWords, activation='softmax')\n",
    "    outputs = denseLayer(decoderOutputs)\n",
    "    model = Model([encoderInputs, decoderInputs], outputs)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = getModel(hiddenDim)\n",
    "model.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['sparse_categorical_accuracy']\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    trainDataGen,\n",
    "    epochs = epochs,\n",
    "    batch_size = batchSize,\n",
    "    validation_data = valDataGen\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"s2s_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model(\"s2s_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeInferenceModels():\n",
    "    embeddingLayer = model.layers[2]\n",
    "\n",
    "    encoderInputs = model.input[0]\n",
    "    encoderEmbedded = embeddingLayer(encoderInputs)\n",
    "    encoderLstm = model.layers[3]\n",
    "    _, hEnc, state_cEnc = encoderLstm(encoderEmbedded)\n",
    "    encoderStates = [hEnc, state_cEnc]\n",
    "    encoderModel = Model(encoderInputs, encoderStates)\n",
    "\n",
    "    decoderInputs = model.input[1]\n",
    "    decoderEmbedded = embeddingLayer(decoderInputs)\n",
    "    hDecInput = Input(shape=(hiddenDim,))\n",
    "    cDecInput = Input(shape=(hiddenDim,))\n",
    "    decoderLstm = model.layers[4]\n",
    "    decoderOutputs, hDec, cDec = decoderLstm(\n",
    "        decoderEmbedded,\n",
    "        initial_state=[hDecInput, cDecInput]\n",
    "    )\n",
    "    decoderDense = model.layers[5]\n",
    "    outputs = decoderDense(decoderOutputs)\n",
    "    decoderModel = Model(\n",
    "        [decoderInputs, hDecInput, cDecInput],\n",
    "        [outputs, hDec, cDec]\n",
    "    )\n",
    "\n",
    "    return encoderModel, decoderModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decodeSequence(inputSeq):\n",
    "    encoderModel, decoderModel = makeInferenceModels()\n",
    "    h, c = encoderModel.predict(inputSeq, verbose=0)\n",
    "\n",
    "    targetSeq = np.zeros((1, 1))\n",
    "    targetSeq[0, 0] = wordToIdx[bos]\n",
    "\n",
    "    decodedSentence = []\n",
    "    for _ in range(1, maxLen):\n",
    "        outputTokens, h, c = decoderModel.predict(\n",
    "            [targetSeq, h, c], verbose=0\n",
    "        )\n",
    "\n",
    "        sampledTokenIndex = np.argmax(outputTokens[0, -1, :])\n",
    "        sampledWord = idxToWord.get(sampledTokenIndex)\n",
    "        if sampledWord is None:\n",
    "            sampledWord = '<OOD>'\n",
    "        decodedSentence.append(sampledWord)\n",
    "\n",
    "        targetSeq = np.zeros((1, 1))\n",
    "        targetSeq[0, 0] = sampledTokenIndex\n",
    "\n",
    "        if sampledWord == eos or len(decodedSentence) > maxLen:\n",
    "            break\n",
    "\n",
    "    return ' '.join(decodedSentence)\n",
    "\n",
    "\n",
    "def respondTo(message):\n",
    "    tokens = tokenizer.texts_to_sequences([message])\n",
    "    sequences = pad_sequences(\n",
    "        tokens,\n",
    "        maxlen = maxLen,\n",
    "        dtype = 'int',\n",
    "        padding = 'post',\n",
    "        truncating = 'post'\n",
    "    )\n",
    "    return decodeSequence(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# respondTo('hi how are you')\n",
    "# respondTo('how does chatgpt work')\n",
    "respondTo('What happens in February')\n",
    "# decodeSequence(enTrain[5:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
